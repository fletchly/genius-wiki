"use strict";(globalThis.webpackChunkgenius_wiki=globalThis.webpackChunkgenius_wiki||[]).push([[1129],{4273:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>h,frontMatter:()=>t,metadata:()=>l,toc:()=>c});const l=JSON.parse('{"id":"setup/configuration/config-yml","title":"config.yml","description":"Global configuration options for Genius","source":"@site/docs/setup/configuration/config-yml.md","sourceDirName":"setup/configuration","slug":"/setup/configuration/config-yml","permalink":"/genius-wiki/docs/next/setup/configuration/config-yml","draft":false,"unlisted":false,"editUrl":"https://github.com/fletchly/genius-wiki/tree/main/docs/setup/configuration/config-yml.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docSidebar","previous":{"title":"Configuration","permalink":"/genius-wiki/docs/next/category/configuration"},"next":{"title":"system-prompt.md","permalink":"/genius-wiki/docs/next/setup/configuration/system-prompt-md"}}');var s=i(2714),o=i(8885);const t={sidebar_position:1},r="config.yml",a={},c=[{value:"genius",id:"genius",level:2},{value:"<code>genius.agentName</code>",id:"geniusagentname",level:3},{value:"<code>genius.agentPrefix</code>",id:"geniusagentprefix",level:3},{value:"<code>genius.playerPrefix</code>",id:"geniusplayerprefix",level:3},{value:"ollama",id:"ollama",level:2},{value:"<code>ollama.baseUrl</code>",id:"ollamabaseurl",level:3},{value:"<code>ollama.apiKey</code>",id:"ollamaapikey",level:3},{value:"<code>ollama.model</code>",id:"ollamamodel",level:3},{value:"<code>ollama.temperature</code>",id:"ollamatemperature",level:3},{value:"<code>ollama.topK</code>",id:"ollamatopk",level:3},{value:"<code>ollama.topP</code>",id:"ollamatopp",level:3},{value:"<code>ollama.numPredict</code>",id:"ollamanumpredict",level:3},{value:"context",id:"context",level:2},{value:"<code>context.maxPlayerMessages</code>",id:"contextmaxplayermessages",level:3}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components},{Details:i}=n;return i||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"configyml",children:"config.yml"})}),"\n",(0,s.jsx)(n.p,{children:"Global configuration options for Genius"}),"\n",(0,s.jsxs)(i,{children:[(0,s.jsx)("summary",{children:"Default config.yml"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# *****************************************\n# *         Genius Configuration          *\n# *****************************************\n# For reference, see https://fletchly.github.io/genius-wiki/docs/setup/configuration/config-yml\n\n# Properties for the agent's appearance and behavior\ngenius:\n  # The name used when displaying messages from the agent in chat\n  agentName: Genius\n\n  # The prefix used when displaying messages from the agent in chat\n  agentPrefix: \ud83d\udca1\n\n  # The prefix used when displaying messages from players in chat\n  playerPrefix: \ud83d\udc64\n# Ollama client configuration\nollama:\n  # The base URL for the Ollama API. If using a cloud model, this should be set to https://ollama.com/.\n  # Defaults to the Ollama API's default listening location. For more info, see https://github.com/fletchly/genius/wiki/hosting\n  baseUrl: http://localhost:11434/\n\n  # Your key for the Ollama cloud API. This only needs to be changed if you are using an Ollama cloud model.\n  # For more info, see https://github.com/fletchly/genius/wiki/hosting\n  apiKey: local\n\n  # The name of the model to use for response generation.\n  # If you are using Ollama cloud, the model must be an ollama cloud model\n  model: deepseek-v3.1:671b\n\n  # Controls how random or deterministic Genius's output is.\n  # Lower values make responses more predictable and precise,\n  # while higher values make them more creative and varied.\n  temperature: 0.5\n\n  # Top-K sampling limits the model\u2019s next-token choices to the K most likely options, filtering out all others.\n  # This reduces randomness while still allowing some creativity compared to always picking the single most probable token.\n  topK: 40\n\n  # Top-P controls how many likely tokens the model considers when generating text.\n  # Lower values make output more focused and predictable, while higher values allow more randomness and creativity.\n  topP: 0.85\n\n  # The number of predictions (max output tokens) controls how much text the model is allowed to generate.\n  # A lower limit keeps responses short and concise, while a higher limit allows longer, more detailed answers.\n  numPredict: 400\n# Context database configuration\ncontext:\n  # Maximum number of messages per player to store in the database at one time\n  # A higher limit will allow players to continue conversations for longer.\n  maxPlayerMessages: 20\n\n# Don't change this\nconfigVersion: 1\n"})})]}),"\n",(0,s.jsx)(n.h2,{id:"genius",children:"genius"}),"\n",(0,s.jsx)(n.p,{children:"Properties for the agent's appearance and behavior"}),"\n",(0,s.jsx)(n.h3,{id:"geniusagentname",children:(0,s.jsx)(n.code,{children:"genius.agentName"})}),"\n",(0,s.jsx)(n.p,{children:"The name used when displaying messages from the agent in chat"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"string"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"Genius"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"geniusagentprefix",children:(0,s.jsx)(n.code,{children:"genius.agentPrefix"})}),"\n",(0,s.jsx)(n.p,{children:"The prefix used when displaying messages from the agent in chat"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"string"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"\ud83d\udca1"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"geniusplayerprefix",children:(0,s.jsx)(n.code,{children:"genius.playerPrefix"})}),"\n",(0,s.jsx)(n.p,{children:"The prefix used when displaying messages from the agent in chat"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"string"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"\ud83d\udc64"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"ollama",children:"ollama"}),"\n",(0,s.jsx)(n.p,{children:"Ollama client configuration"}),"\n",(0,s.jsx)(n.h3,{id:"ollamabaseurl",children:(0,s.jsx)(n.code,{children:"ollama.baseUrl"})}),"\n",(0,s.jsxs)(n.p,{children:["The base URL for the Ollama API. If using a cloud model, this should be set to ",(0,s.jsx)(n.code,{children:"https://ollama.com/"}),". Defaults to the Ollama API's default listening location. For more info, see ",(0,s.jsx)(n.a,{href:"/genius-wiki/docs/next/setup/hosting#choosing-a-hosting-strategy",children:"the hosting guide"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"string"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"http://localhost:11434/"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ollamaapikey",children:(0,s.jsx)(n.code,{children:"ollama.apiKey"})}),"\n",(0,s.jsxs)(n.p,{children:["Your key for the Ollama cloud API. This only needs to be set if you are using an Ollama cloud model. For more info, see ",(0,s.jsx)(n.a,{href:"/genius-wiki/docs/next/setup/hosting#choosing-a-hosting-strategy",children:"the hosting guide"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"string"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"(blank)"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ollamamodel",children:(0,s.jsx)(n.code,{children:"ollama.model"})}),"\n",(0,s.jsxs)(n.p,{children:["The name of the model to use for response generation. If you are using Ollama cloud, the model ",(0,s.jsx)(n.em,{children:"must"})," be an ",(0,s.jsx)(n.a,{href:"https://docs.ollama.com/cloud#cloud-models",children:"ollama cloud model"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"string"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"deepseek-v3.1:671b"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ollamatemperature",children:(0,s.jsx)(n.code,{children:"ollama.temperature"})}),"\n",(0,s.jsx)(n.p,{children:"Controls how random or deterministic Genius's output is. Lower values make responses more predictable and precise, while higher values make them more creative and varied."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"number"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"0.5"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ollamatopk",children:(0,s.jsx)(n.code,{children:"ollama.topK"})}),"\n",(0,s.jsx)(n.p,{children:"Top-K sampling limits the model\u2019s next-token choices to the K most likely options, filtering out all others. This reduces randomness while still allowing some creativity compared to always picking the single most probable token."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"integer"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"40"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ollamatopp",children:(0,s.jsx)(n.code,{children:"ollama.topP"})}),"\n",(0,s.jsx)(n.p,{children:"Top-P controls how many likely tokens the model considers when generating text. Lower values make output more focused and predictable, while higher values allow more randomness and creativity."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"number"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"0.85"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ollamanumpredict",children:(0,s.jsx)(n.code,{children:"ollama.numPredict"})}),"\n",(0,s.jsx)(n.p,{children:"The number of predictions (max output tokens) controls how much text the model is allowed to generate. A lower limit keeps responses short and concise, while a higher limit allows longer, more detailed answers."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"integer"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"400"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"context",children:"context"}),"\n",(0,s.jsx)(n.p,{children:"Conversation context configuration"}),"\n",(0,s.jsx)(n.h3,{id:"contextmaxplayermessages",children:(0,s.jsx)(n.code,{children:"context.maxPlayerMessages"})}),"\n",(0,s.jsx)(n.p,{children:"Maximum number of messages to store per player. A higher limit will allow players to continue conversations for longer, however this could cause performance impacts with high player counts."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"integer"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"20"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8885:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>r});var l=i(9378);const s={},o=l.createContext(s);function t(e){const n=l.useContext(o);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),l.createElement(o.Provider,{value:n},e.children)}}}]);