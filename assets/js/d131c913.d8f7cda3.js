"use strict";(globalThis.webpackChunkgenius_wiki=globalThis.webpackChunkgenius_wiki||[]).push([[126],{2570:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>h,frontMatter:()=>t,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"setup/configuration","title":"Configuration","description":"Configure the Genius plugin","source":"@site/docs/setup/configuration.md","sourceDirName":"setup","slug":"/setup/configuration","permalink":"/docs/next/setup/configuration","draft":false,"unlisted":false,"editUrl":"https://github.com/fletchly/genius-wiki/tree/main/docs/setup/configuration.md","tags":[],"version":"current","frontMatter":{"sidebar_postition":2},"sidebar":"docSidebar","previous":{"title":"Installation","permalink":"/docs/next/setup/installation"},"next":{"title":"Hosting","permalink":"/docs/next/setup/hosting"}}');var s=l(2714),i=l(8885);const t={sidebar_postition:2},r="Configuration",a={},c=[{value:"genius",id:"genius",level:2},{value:"genius.agentName",id:"geniusagentname",level:3},{value:"ollama",id:"ollama",level:2},{value:"ollama.baseUrl",id:"ollamabaseurl",level:3},{value:"ollama.apiKey",id:"ollamaapikey",level:3},{value:"ollama.model",id:"ollamamodel",level:3},{value:"ollama.temperature",id:"ollamatemperature",level:3},{value:"ollama.topK",id:"ollamatopk",level:3},{value:"ollama.topP",id:"ollamatopp",level:3},{value:"ollama.numPredict",id:"ollamanumpredict",level:3},{value:"context",id:"context",level:2},{value:"context.maxPlayerMessages",id:"contextmaxplayermessages",level:3}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components},{Details:l}=n;return l||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"configuration",children:"Configuration"})}),"\n",(0,s.jsx)(n.p,{children:"Configure the Genius plugin"}),"\n",(0,s.jsxs)(l,{children:[(0,s.jsx)("summary",{children:"Default config.yml"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# *****************************************\n# *         Genius Configuration          *\n# *****************************************\n# For reference, see https://github.com/fletchly/genius/wiki/Configuration\n\n# Properties for the agent's appearance and behavior\ngenius:\n  # The name for the Genius agent to use in chat when responding to messages\n  agentName: Genius\n\n# Ollama client configuration\nollama:\n  # The base URL for the Ollama API. If using a cloud model, this should be set to https://ollama.com/.\n  # Defaults to the Ollama API's default listening location. For more info, see https://github.com/fletchly/genius/wiki/hosting\n  baseUrl: http://localhost:11434/\n\n  # Your key for the Ollama cloud API. This only needs to be set if you are using an Ollama cloud model.\n  # For more info, see https://github.com/fletchly/genius/wiki/hosting\n  apiKey: API_KEY\n\n  # The name of the model to use for response generation.\n  # If you are using Ollama cloud, the model must be an ollama cloud model\n  model: deepseek-v3.1:671b\n\n  # Controls how random or deterministic Genius's output is.\n  # Lower values make responses more predictable and precise,\n  # while higher values make them more creative and varied.\n  temperature: 0.5\n\n  # Top-K sampling limits the model\u2019s next-token choices to the K most likely options, filtering out all others.\n  # This reduces randomness while still allowing some creativity compared to always picking the single most probable token.\n  topK: 40\n\n  # Top-P controls how many likely tokens the model considers when generating text.\n  # Lower values make output more focused and predictable, while higher values allow more randomness and creativity.\n  topP: 0.85\n\n  # The number of predictions (max output tokens) controls how much text the model is allowed to generate.\n  # A lower limit keeps responses short and concise, while a higher limit allows longer, more detailed answers.\n  numPredict: 400\n\n# Conversation context configuration\ncontext:\n  # Maximum number of messages to store per player\n  # A higher limit will allow players to continue conversations for longer,\n  # however this could cause performance impacts with high player counts.\n  maxPlayerMessages: 20\n"})})]}),"\n",(0,s.jsx)(n.h2,{id:"genius",children:"genius"}),"\n",(0,s.jsx)(n.p,{children:"Properties for the agent's appearance and behavior"}),"\n",(0,s.jsx)(n.h3,{id:"geniusagentname",children:"genius.agentName"}),"\n",(0,s.jsx)(n.p,{children:"The name for the Genius agent to use in chat when responding to messages"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"string"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"Genius"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"ollama",children:"ollama"}),"\n",(0,s.jsx)(n.p,{children:"Ollama client configuration"}),"\n",(0,s.jsx)(n.h3,{id:"ollamabaseurl",children:"ollama.baseUrl"}),"\n",(0,s.jsxs)(n.p,{children:["The base URL for the Ollama API. If using a cloud model, this should be set to ",(0,s.jsx)(n.code,{children:"https://ollama.com/"}),". Defaults to the Ollama API's default listening location. For more info, see ",(0,s.jsx)(n.a,{href:"/docs/next/setup/hosting#choosing-a-hosting-strategy",children:"the hosting guide"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"string"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"http://localhost:11434/"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ollamaapikey",children:"ollama.apiKey"}),"\n",(0,s.jsxs)(n.p,{children:["Your key for the Ollama cloud API. This only needs to be set if you are using an Ollama cloud model. For more info, see ",(0,s.jsx)(n.a,{href:"/docs/next/setup/hosting#choosing-a-hosting-strategy",children:"the hosting guide"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"string"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"(blank)"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ollamamodel",children:"ollama.model"}),"\n",(0,s.jsxs)(n.p,{children:["The name of the model to use for response generation. If you are using Ollama cloud, the model ",(0,s.jsx)(n.em,{children:"must"})," be an ",(0,s.jsx)(n.a,{href:"https://docs.ollama.com/cloud#cloud-models",children:"ollama cloud model"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"string"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"deepseek-v3.1:671b"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ollamatemperature",children:"ollama.temperature"}),"\n",(0,s.jsx)(n.p,{children:"Controls how random or deterministic Genius's output is. Lower values make responses more predictable and precise, while higher values make them more creative and varied."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"number"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"0.5"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ollamatopk",children:"ollama.topK"}),"\n",(0,s.jsx)(n.p,{children:"Top-K sampling limits the model\u2019s next-token choices to the K most likely options, filtering out all others. This reduces randomness while still allowing some creativity compared to always picking the single most probable token."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"integer"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"40"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ollamatopp",children:"ollama.topP"}),"\n",(0,s.jsx)(n.p,{children:"Top-P controls how many likely tokens the model considers when generating text. Lower values make output more focused and predictable, while higher values allow more randomness and creativity."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"number"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"0.85"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ollamanumpredict",children:"ollama.numPredict"}),"\n",(0,s.jsx)(n.p,{children:"The number of predictions (max output tokens) controls how much text the model is allowed to generate. A lower limit keeps responses short and concise, while a higher limit allows longer, more detailed answers."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"integer"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"400"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"context",children:"context"}),"\n",(0,s.jsx)(n.p,{children:"Conversation context configuration"}),"\n",(0,s.jsx)(n.h3,{id:"contextmaxplayermessages",children:"context.maxPlayerMessages"}),"\n",(0,s.jsx)(n.p,{children:"Maximum number of messages to store per player. A higher limit will allow players to continue conversations for longer, however this could cause performance impacts with high player counts."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Type:"})," ",(0,s.jsx)(n.code,{children:"integer"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default:"})," ",(0,s.jsx)(n.code,{children:"20"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8885:(e,n,l)=>{l.d(n,{R:()=>t,x:()=>r});var o=l(9378);const s={},i=o.createContext(s);function t(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);