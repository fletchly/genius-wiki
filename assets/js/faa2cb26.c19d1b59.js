"use strict";(globalThis.webpackChunkgenius_wiki=globalThis.webpackChunkgenius_wiki||[]).push([[3426],{3638:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>t,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"setup/configuration/genius-conf","title":"genius.conf","description":"Global configuration options for Genius","source":"@site/docs/setup/configuration/genius-conf.md","sourceDirName":"setup/configuration","slug":"/setup/configuration/genius-conf","permalink":"/genius-wiki/docs/next/setup/configuration/genius-conf","draft":false,"unlisted":false,"editUrl":"https://github.com/fletchly/genius-wiki/tree/main/docs/setup/configuration/genius-conf.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docSidebar","previous":{"title":"Configuration","permalink":"/genius-wiki/docs/next/category/configuration"},"next":{"title":"system-prompt.md","permalink":"/genius-wiki/docs/next/setup/configuration/system-prompt-md"}}');var l=s(2714),o=s(8885);const r={sidebar_position:1},t="genius.conf",a={},d=[{value:"display",id:"display",level:2},{value:"<code>display.agent-name</code>",id:"displayagent-name",level:3},{value:"<code>display.agent-prefix</code>",id:"displayagent-prefix",level:3},{value:"<code>display.player-prefix</code>",id:"displayplayer-prefix",level:3},{value:"ollama",id:"ollama",level:2},{value:"<code>ollama.base-url</code>",id:"ollamabase-url",level:3},{value:"<code>ollama.api-key</code>",id:"ollamaapi-key",level:3},{value:"<code>ollama.model</code>",id:"ollamamodel",level:3},{value:"<code>ollama.temperature</code>",id:"ollamatemperature",level:3},{value:"<code>ollama.top-k</code>",id:"ollamatop-k",level:3},{value:"<code>ollama.top-p</code>",id:"ollamatop-p",level:3},{value:"<code>ollama.num-predict</code>",id:"ollamanum-predict",level:3},{value:"context",id:"context",level:2},{value:"<code>context.max-player-messages</code>",id:"contextmax-player-messages",level:3},{value:"version",id:"version",level:2}];function c(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components},{Details:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"geniusconf",children:"genius.conf"})}),"\n",(0,l.jsx)(n.p,{children:"Global configuration options for Genius"}),"\n",(0,l.jsxs)(s,{children:[(0,l.jsx)("summary",{children:"Default genius.conf"}),(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-hocon",metastring:'title="/plugins/Genius/genius.conf"',children:'# *****************************************\n# *         Genius Configuration          *\n# *****************************************\n# For reference, see https://fletchly.github.io/genius-wiki/docs/setup/configuration/genius-conf\n\n# Properties for the agent\'s appearance and behavior\ndisplay {\n  # The name used when displaying messages from the agent in chat\n  agent-name=Genius\n  # The prefix used when displaying messages from the agent in chat\n  agent-prefix="\ud83d\udca1"\n  # The prefix used when displaying messages from players in chat\n  player-prefix="\ud83d\udc64"\n}\n# Ollama client configuration\nollama {\n  # The base URL for the Ollama API. For more info, see https://fletchly.github.io/genius-wiki/docs/setup/hosting\n  base-url="https://ollama.com"\n  # Your key for the Ollama cloud API. This only needs to be set if you are using an Ollama cloud model.\n  api-key=""\n  # The name of the model to use for response generation.\n  model="deepseek-v3.1:671b"\n  # Controls how random or deterministic the output is.\n  temperature=0.5\n  # Restricts sampling to the K most probable next tokens, making output more focused (low values) or more creative (high values).\n  top-k=40\n  # Limits sampling to the smallest group of likely next tokens that together reach probability P, for more focused (low P) or creative (high P) output.\n  top-p=0.85\n  # Sets the maximum number of tokens the model can generate in its response (higher values allow longer outputs; lower values keep them shorter)\n  num-predict=400\n}\n# Context store configuration\ncontext {\n  # Maximum number of messages per player to store at one time\n  max-player-messages=20\n}\n# Don\'t change this. Doing so could overwrite existing config\nversion=0\n'})})]}),"\n",(0,l.jsx)(n.h2,{id:"display",children:"display"}),"\n",(0,l.jsx)(n.p,{children:"Properties for the agent's appearance and behavior"}),"\n",(0,l.jsx)(n.h3,{id:"displayagent-name",children:(0,l.jsx)(n.code,{children:"display.agent-name"})}),"\n",(0,l.jsx)(n.p,{children:"The name used when displaying messages from the agent in chat"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Type:"})," ",(0,l.jsx)(n.code,{children:"string"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Default:"})," ",(0,l.jsx)(n.code,{children:"Genius"})]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"displayagent-prefix",children:(0,l.jsx)(n.code,{children:"display.agent-prefix"})}),"\n",(0,l.jsx)(n.p,{children:"The prefix used when displaying messages from the agent in chat"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Type:"})," ",(0,l.jsx)(n.code,{children:"string"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Default:"})," ",(0,l.jsx)(n.code,{children:"\ud83d\udca1"})]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"displayplayer-prefix",children:(0,l.jsx)(n.code,{children:"display.player-prefix"})}),"\n",(0,l.jsx)(n.p,{children:"The prefix used when displaying messages from the agent in chat"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Type:"})," ",(0,l.jsx)(n.code,{children:"string"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Default:"})," ",(0,l.jsx)(n.code,{children:"\ud83d\udc64"})]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"ollama",children:"ollama"}),"\n",(0,l.jsx)(n.p,{children:"Ollama client configuration"}),"\n",(0,l.jsx)(n.h3,{id:"ollamabase-url",children:(0,l.jsx)(n.code,{children:"ollama.base-url"})}),"\n",(0,l.jsxs)(n.p,{children:["The base URL for the Ollama API. If using a cloud model, this should be set to ",(0,l.jsx)(n.code,{children:"https://ollama.com/"}),". Defaults to the Ollama API's default listening location. For more info, see ",(0,l.jsx)(n.a,{href:"/genius-wiki/docs/next/setup/hosting#choosing-a-hosting-strategy",children:"the hosting guide"})]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Type:"})," ",(0,l.jsx)(n.code,{children:"string"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Default:"})," ",(0,l.jsx)(n.code,{children:"http://localhost:11434/"})]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"ollamaapi-key",children:(0,l.jsx)(n.code,{children:"ollama.api-key"})}),"\n",(0,l.jsxs)(n.p,{children:["Your key for the Ollama cloud API. This only needs to be set if you are using an Ollama cloud model. For more info, see ",(0,l.jsx)(n.a,{href:"/genius-wiki/docs/next/setup/hosting#choosing-a-hosting-strategy",children:"the hosting guide"})]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Type:"})," ",(0,l.jsx)(n.code,{children:"string"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Default:"})," ",(0,l.jsx)(n.code,{children:"(blank)"})]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"ollamamodel",children:(0,l.jsx)(n.code,{children:"ollama.model"})}),"\n",(0,l.jsxs)(n.p,{children:["The name of the model to use for response generation. If you are using Ollama cloud, the model ",(0,l.jsx)(n.em,{children:"must"})," be an ",(0,l.jsx)(n.a,{href:"https://docs.ollama.com/cloud#cloud-models",children:"ollama cloud model"})]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Type:"})," ",(0,l.jsx)(n.code,{children:"string"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Default:"})," ",(0,l.jsx)(n.code,{children:"deepseek-v3.1:671b"})]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"ollamatemperature",children:(0,l.jsx)(n.code,{children:"ollama.temperature"})}),"\n",(0,l.jsx)(n.p,{children:"Controls how random or deterministic Genius's output is. Lower values make responses more predictable and precise, while higher values make them more creative and varied."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Type:"})," ",(0,l.jsx)(n.code,{children:"number"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Default:"})," ",(0,l.jsx)(n.code,{children:"0.5"})]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"ollamatop-k",children:(0,l.jsx)(n.code,{children:"ollama.top-k"})}),"\n",(0,l.jsx)(n.p,{children:"Top-K sampling limits the model\u2019s next-token choices to the K most likely options, filtering out all others. This reduces randomness while still allowing some creativity compared to always picking the single most probable token."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Type:"})," ",(0,l.jsx)(n.code,{children:"integer"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Default:"})," ",(0,l.jsx)(n.code,{children:"40"})]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"ollamatop-p",children:(0,l.jsx)(n.code,{children:"ollama.top-p"})}),"\n",(0,l.jsx)(n.p,{children:"Top-P controls how many likely tokens the model considers when generating text. Lower values make output more focused and predictable, while higher values allow more randomness and creativity."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Type:"})," ",(0,l.jsx)(n.code,{children:"number"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Default:"})," ",(0,l.jsx)(n.code,{children:"0.85"})]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"ollamanum-predict",children:(0,l.jsx)(n.code,{children:"ollama.num-predict"})}),"\n",(0,l.jsx)(n.p,{children:"The number of predictions (max output tokens) controls how much text the model is allowed to generate. A lower limit keeps responses short and concise, while a higher limit allows longer, more detailed answers."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Type:"})," ",(0,l.jsx)(n.code,{children:"integer"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Default:"})," ",(0,l.jsx)(n.code,{children:"400"})]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"context",children:"context"}),"\n",(0,l.jsx)(n.p,{children:"Context store configuration"}),"\n",(0,l.jsx)(n.h3,{id:"contextmax-player-messages",children:(0,l.jsx)(n.code,{children:"context.max-player-messages"})}),"\n",(0,l.jsx)(n.p,{children:"Maximum number of messages to store per player. A higher limit will allow players to continue conversations for longer, however this could cause performance impacts with high player counts."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Type:"})," ",(0,l.jsx)(n.code,{children:"integer"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Default:"})," ",(0,l.jsx)(n.code,{children:"20"})]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"version",children:"version"}),"\n",(0,l.jsx)(n.p,{children:"Config version used internally by Genius."}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Do not change this value."})," Any changes will likely result in your config getting overwritten/not updating correctly in the future."]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(c,{...e})}):c(e)}},8885:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>t});var i=s(9378);const l={},o=i.createContext(l);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);